{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap02 - 시작하기 전에: 신경망의 수학적 구성요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 내용\n",
    "\n",
    "- 첫 번째 신경망 예제 만들기\n",
    "- 텐서와 텐서 연산의 개념\n",
    "- 역전파와 경사 하강법을 사용하여 신경망이 학습되는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 신경망과의 첫 만남"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스 파이썬 라이브러리를 사용하여 손글씨 숫자 분류를 학습하는 신경망 예제를 살펴봅니다.\n",
    "\n",
    "데이터셋은 MNIST를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 샘플 이미지\n",
    "\n",
    "![](./images/2.1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 데이터로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape : (60000, 28, 28)\n",
      "train_labels.shape : (60000,)\n",
      "test_images.shape : (10000, 28, 28)\n",
      "test_labels.shape : (10000,)\n"
     ]
    }
   ],
   "source": [
    "#data-load\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print('train_images.shape :', train_images.shape)\n",
    "print('train_labels.shape :', train_labels.shape)\n",
    "print('test_images.shape :', test_images.shape)\n",
    "print('test_labels.shape :', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = train_images[0]\n",
    "plt.imshow(digit, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape : (60000, 784)\n",
      "train_labels.shape : (60000,)\n",
      "test_images.shape : (10000, 784)\n",
      "test_labels.shape : (10000,)\n"
     ]
    }
   ],
   "source": [
    "# pre-processing\n",
    "train_images = train_images.reshape([-1, 28*28])\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape([-1, 28*28])\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "print('train_images.shape :', train_images.shape)\n",
    "print('train_labels.shape :', train_labels.shape)\n",
    "print('test_images.shape :', test_images.shape)\n",
    "print('test_labels.shape :', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# model\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **소프트맥스(softmax)**층은 10개의 확률 점수가 들어 있는 배열(모두 더하면 1이다)을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **손실 함수(loss function)** : 훈련 데이터에서 신경망의 성능을 측정하는 방법으로 네트워크가 옳은 방향으로 학습되도록 한다.\n",
    "- **옵티마이저(optimizer)** : 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘이다.\n",
    "- **훈련과 네스트 과정을 모니터링할 지표** : 여기에서는 정확도만 고려한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.2570 - acc: 0.9258\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.1042 - acc: 0.9692\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0691 - acc: 0.9793\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0500 - acc: 0.9852\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0376 - acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dadd79908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 179us/step\n"
     ]
    }
   ],
   "source": [
    "#evaluating\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.9794\n",
      "test_loss : 0.0687\n"
     ]
    }
   ],
   "source": [
    "print('test_acc', test_acc)\n",
    "print('test_loss : {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 세트의 정확도는 97.9%로 훈련세트 정확도보다는 낮다.\n",
    "\n",
    "이 차이는 **과대적합(overfitting)** 때문이다.\n",
    "\n",
    "이는 머신 러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 신경망을 위한 데이터 표현\n",
    "\n",
    "- 거의 모든 딥러닝 프레임워크는 일반적으로 **텐서(Tensor)**를 기본 데이터 구조로 사용한다.\n",
    "- 텐서는 데이터를 위한 **컨테이너**(container)이다.\n",
    "- 텐서는 임의의 차원(또는 축) 개수를 가지는 행렬의 일반화된 모습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 스칼라(Scalar, 0D tensor)\n",
    "\n",
    "- 하나의 숫자만 담고 있는 텐서를 **스칼라**(scalar, 또는 스칼라 텐서, 0차원 텐서, 0D 텐서)라고 부른다.\n",
    "- 스칼라 텐서의 차원(축 개수)은 0이다.\n",
    "- NumPy의 `.ndim` 속성을 이용해 차원(축)을 확인할 수 있다. \n",
    "- 텐서의 축 개수를 **랭크(rank)**라고도 부른다.\n",
    "  - 여기서 랭크는 선형대수에서 선형 독립의 개수를 의미하는 계수(rank)와는 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  12\n",
      "x.ndim :  0\n",
      "x.shape : ()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array(12)\n",
    "print('x : ',x)\n",
    "print('x.ndim : ',x.ndim)\n",
    "print('x.shape :', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 벡터 (1D tensor)\n",
    "\n",
    "- 숫자의 배열을 **벡터(vector)** 또는 1D 텐서라고 부른다. \n",
    "- 1D 텐서는 딱 하나의 축(axis)을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12  3  6 14  7]\n",
      "x.ndim :  1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14, 7])\n",
    "print(x)\n",
    "print('x.ndim : ', x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 벡터는 5개의 원소를 가지고 있으므로 5차원 벡터라고 부른다.\n",
    "- 벡터는 하나의 축을 따라 $N$-개의 차원을 가진 것이고, 텐서는 $N$-개의 축을 가진것이다. 텐서의 각 축을 따라 여러 개의 차원을 가진 벡터가 놓일 수 있다.\n",
    "  - 예를들어, RGB 이미지는 3D 텐서이며, 채널(channel)이란 축을 따라 R, G, B에 해당하는 2D 텐서 즉, Matrix를 가진다.\n",
    "- **차원수(dimensionality)**는 벡터 개념에서의 차원처럼 각 축을 따라 놓인 원소의 개수를 의미하기 때문에, 텐서에서는 랭크를 사용하는 것이 기술적으로 좀더 정확하다.\n",
    "  - 예를들어, 5D 텐서의 경우 5차원 텐서 보다는 랭크 5인 텐서라고 말하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 행렬 (2D tensor)\n",
    "\n",
    "- 벡터의 배열을 **행렬**(matrix) 또는 2D 텐서라고 한다. \n",
    "- 즉, 행렬에는 2개의 축이 있다. (보통 행(row)과 열(column)이라고 부른다.)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 5 & 78 & 2 & 34 & 0 \\\\ 6 & 79 & 3 & 35 & 1 \\\\ 7 & 80 & 4 & 36 & 2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 3D 텐서와 고차원 텐서\n",
    "\n",
    "- 행렬들을 하나의 새로운 배열로 합치면 직육면체의 형태로 해석할 수 있는 3D 텐서가 만들어진다. \n",
    "- 3D 텐서들을 하나의 배열로 합치면 4D 텐서가 되는데, 이런식으로 고차원 텐서를 만든다.\n",
    "    - 2D 텐서들을 하나의 배열로 합쳐서 3D 텐서가 되었다.\n",
    "- 딥러닝에서는 보통 0D ~ 4D 까지의 텐서를 다룬다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nx.ndim :  3\n",
      "x.shape : (3, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0], \n",
    "               [6, 79, 3, 35, 1], \n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0], \n",
    "               [6, 79, 3, 35, 1], \n",
    "               [7, 80, 4, 36, 2]], \n",
    "              [[5, 78, 2, 34, 0], \n",
    "               [6, 79, 3, 35, 1], \n",
    "               [7, 80, 4, 36, 2]]])\n",
    "print('nx.ndim : ',x.ndim)\n",
    "print('x.shape :', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 핵심 속성\n",
    "\n",
    "- 텐서는 3개의 핵심 속성으로 정의된다.\n",
    "  - **축의 개수(rank)** : 예를 들어 3D 텐서에는 3개의 축이 있고, 행렬에는 2개의 축이 있다. NumPy의 `.ndim` 을 통해 확인할 수 있다.\n",
    "  - **크기(shape)** : 텐서의 각 축을  따라 얼마나 많은 차원이 있는지를 나타내는 Python의 tuple이다. NumPy의 `.shape`을 통해 확인할 수 있다.\n",
    "  - **데이터 타입(data type)** : 텐서에 포함된 데이터의 타입이다.\n",
    "    - `.dtype`을 통해 확인 가능하다.\n",
    "    - `float32, float64, uint8, int32`등\n",
    "    - 텐서는 사전에 할당되어 메모리에 저장되어야 하기 때문에 가변 길이의 문자열을 지원하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 NumPy로 텐서 조작하기\n",
    "\n",
    "- 배열에 있는 특정 원소들을 선택해는 것을 **슬라이싱(slicing)** 이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 784)\n"
     ]
    }
   ],
   "source": [
    "# 11번째에서 101번째 까지 (101번째는 포함x)\n",
    "my_slice = test_images[10:100]\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 784)\n",
      "(90, 784)\n",
      "(90, 280)\n"
     ]
    }
   ],
   "source": [
    "my_slice = test_images[10:100, :]\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice = test_images[10:100, 0: 784]\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice = test_images[10:100, 500: 780]\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 배치 데이터\n",
    "\n",
    "- 일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축은 **샘플 축(sample axis)** 이라고 한다.\n",
    "- 딥러닝 모델은 한 번에 전체 데이터셋을 가지고 학습하지 않고, 전체 데이터셋에서 일부 데이터를 샘플링후 학습한다. 이를 **배치(batch)** 라고 한다.\n",
    "- 배치 데이터에서 첫 번째 축(0번 축)을 **배치 축(batch axis)** 또는 **배치 차원(batch dimension)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "batch = train_images[:128]\n",
    "\n",
    "그 다음 배치는 \n",
    "\n",
    "batch = train_images[128:256]\n",
    "\n",
    "n번째 배치는 \n",
    "\n",
    "batch = train_images[128*n : 128*(n+1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.8 텐서의 실제 사례\n",
    "\n",
    "딥러닝에서 자주 사용할 데이터는 다음과 같은 것들이 있다.\n",
    "\n",
    "- **벡터 데이터** : `(samples, features)` 형태의 2D 텐서\n",
    "    - (나이, 우편번호, 소득)으로 구성된 통계 데이터, `(N, 3)`\n",
    "- **시계열 데이터** 또는 **시퀀스 데이터** : `(samples, timesteps, features)` 형태의 3D 텐서\n",
    "    - 주식 가격 데이터셋은 (250일치, 하루거래시간(분), (현재, 최소, 최고)) → `(250, 390, 3)` \n",
    "- **이미지** : `(samples, height, width, channels)` 또는 `(samples, channels, height, width)` 형태의 4D 텐서\n",
    "    - MNIST 데이터 셋 (배치, 높이, 너비, 채널) → `(N, 28, 28, 1)` \n",
    "- **동영상** : `(samples, frames, height, width, channels)` 또는 `(samples, frames, channels, height, width)` 형태의 5D 텐서\n",
    "    - 60초 짜리 144$\\times$256 유튜브 비디오 클립을 초당 4프레임으로 샘플링하면 240 프레임이며, 이런 비디오 클립을 4개 가진 배치는 `(4, 240, 144, 256, 3)` 텐서에 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 신경망의 톱니바퀴: 텐서 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 원소별(element-wise) 연산\n",
    "\n",
    "- 원소별 연산(element-wise operation)은 텐서에 있는 각 원소에 독립적으로 적용되는 연산을 말한다. \n",
    "- NumPy는 텐서의 원소별 연산을 제공하여 빠르게 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(x) :\n",
      "[[0.  0.1]\n",
      " [0.  3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)    # relu 연산은 입력이 0보다크면 입력그대로를 반환, 0보다 작으면 0을 반환한다.\n",
    "\n",
    "x = np.array([[-1, 0.1], \n",
    "              [-0.2, 3]])\n",
    "\n",
    "print('relu(x) :\\n{}'.format(relu(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 브로드캐스팅 (Broadcasting)\n",
    "\n",
    "- Broadcast의 사전적인 의미는 '퍼뜨리다'라는 뜻이 있는데, 이와 마찬가지로 두 텐서 A, B 중 크기가 작은 텐서를 크기가 큰 텐서와 형태(shape)가 맞게끔 늘려주는 것을 의미한다.\n",
    "\n",
    "- 브로드캐스팅은 두 단계로 이루어진다.\n",
    "\n",
    "  - ① 큰 텐서의 랭크(차원)에 맞도록 작은 텐서에 축(axis)이 추가된다.\n",
    "  - ② 작은 텐서가 새 축을 따라서 큰 텐서의 형태에 맞도록 반복된다.\n",
    "\n",
    "![broadcasting](./images/broadcasting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. 텐서 점곱 (tensor dot)\n",
    "\n",
    "- **텐서 곱셈**(tensor product)라고도 한다.\n",
    "\n",
    "- NumPy, Keras에서는 `.dot`을 이용해 텐서 곱셈을 수행한다.\n",
    "- 고차원 텐서 간의 곱셈은 다음과 같이 할 수 있다.\n",
    "  - `(a, b, c, d) ∙ (d,) → (a, b, c)`\n",
    "  - `(a, b, c, d) ∙ (d, e) → (a, b, c, e) `  \n",
    "\n",
    "![](./images/dot.PNG)\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\cdot \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}=\\begin{bmatrix} 17 & 22 & 27 \\\\ 22 & 29 & 36 \\\\ 27 & 36 & 45 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape : (3, 2), y.shape : (2, 3)\n",
      "z.shape : (3, 3)\n",
      "[[17 22 27]\n",
      " [22 29 36]\n",
      " [27 36 45]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 4], \n",
    "              [2, 5], \n",
    "              [3, 6]])\n",
    "y = np.array([[1, 2, 3], \n",
    "              [4, 5, 6]])\n",
    "\n",
    "z = np.dot(x, y)\n",
    "\n",
    "print('x.shape : {}, y.shape : {}'.format(x.shape, y.shape))\n",
    "print('z.shape : {}\\n{}'.format(z.shape, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 텐서 형태 변환\n",
    "\n",
    "- 첫번째 예제에서 신경망에 주입할 숫자 데이터를 전처리할 때 사용했다.\n",
    "\n",
    "```python\n",
    "train_images = train_images.reshape((60000,28 * 28))\n",
    "```\n",
    "\n",
    "- 텐서 형태 변환(tensor reshaping)은 특정 형태에 맞게 행과 열을 재배열 한다는 뜻이다.\n",
    "- NumPy의 `.reshape()`을 이용해 형태를 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "x.shape : (6, 1)\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "x.shape : (2, 3)\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "np.transpose(x) : (3, 2)\n",
      "[[0. 3.]\n",
      " [1. 4.]\n",
      " [2. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[0., 1.], \n",
    "              [2., 3.], \n",
    "              [4., 5.]])\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = x.reshape(6,1)\n",
    "print('x.shape : {}\\n{}'.format(x.shape, x))\n",
    "\n",
    "x = x.reshape(2,3) \n",
    "print('x.shape : {}\\n{}'.format(x.shape, x))\n",
    "\n",
    "x = np.transpose(x)  \n",
    "print('np.transpose(x) : {}\\n{}'.format(x.shape, x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 텐서 연산의 기하학적 해석\n",
    "\n",
    "- 텐서 연산이 조작하는 텐서는 기하학적 공간의 좌표 포인트로 해석될 수 있기때문에 모든 텐서 연산은 기하학적 해석이 가능하다.\n",
    "- 텐서 연산은 일반적으로 아핀변환(affine transformation) 회전, 스케일링(scaling) 등 기본적인 기하학적 연산을 표현할 수 있다.\n",
    "- **Affine transformation** : 아핀 변환은 점, 직선, 평면을 보존하는 아핀 공간으로의 변환을 의미한다. 이 변환은 거리의 비율과 직선의 평행을 유지하는 이동, 스케일링, 회전 등이 포함된다.\n",
    "  - Affine Space : 아핀공간은 벡터공간을 평행이동한 것이라 할 수 있다. \n",
    "\n",
    "\n",
    "<img src=\"./images/2.8.png\" height=\"80%\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 그래디언트 기반 최적화\n",
    "\n",
    "- 신경망 예제에 있는 각 층은 입력 데이터를 다음과 같이 변환한다.\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) * b)\n",
    "```\n",
    "\n",
    "- 딥러닝 모델의 학습 초기에는 가중치($\\mathbf{W}$) 파라미터는 랜덤하게 초기화된다.\n",
    "- 학습이 진행되면서 **역전파(backpropagation)**에 의해 가중치가 업데이트 된다.\n",
    "- 훈련은 다음과 같은 단계가 반복된다.\n",
    "\n",
    "\n",
    "1. 먼저, 각 학습 데이터 샘플을 네트워크에 입력으로 넣어주고 출력층까지 각 층의 뉴런 마다 출력을 계산한다. 이를 **순전파**(forward propagation)이라고 한다. \n",
    "\n",
    "2. 그 다음 네트워크의 마지막 출력층에 대한 결과(예측값)와 실제값과의 차이, 즉 오차(error)를 계산하는데, 손실함수(loss function)를 이용하여 계산한다.\n",
    "\n",
    "3. 그리고 이 오차를 역방향으로 흘러 보내면서, 각 출력 뉴런의 오차에 마지막 입력 뉴런이 얼마나 기여했는지 측정한다. 이말을 쉽게 설명하면, **각 뉴런의 입력값에 대한 손실함수의 편미분, 그래디언트(gradient)을 계산**하는 것을 말한다.\n",
    "\n",
    "4. 3번과 같은 방법을 입력층에 도달할 때까지 계속 반복해서 역방향으로 흘러 보낸다.\n",
    "\n",
    "5. 마지막으로, 계산한 그래디언트를 네트워크의 모든 가중치 매개변수에 반영해주는 **경사 하강법 단계**를 수행한다.\n",
    "\n",
    "- **그래디언트**(gradient)는 텐서 연산의 변화율을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 변화율\n",
    "\n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{h \\rightarrow 0}{\\frac{f(x+h) - f(x)}{h}}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"./images/2.10.png\" height=\"40%\" width=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 텐서의 변화율 : 그래디언트\n",
    "\n",
    "- 그래디언트(gradient)는 텐서 연산의 변화율을 말한다. \n",
    "- 다차원 입력, 즉 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것이다.\n",
    "\n",
    "```python\n",
    "y_perd = dot(W, x)\n",
    "loss_value = loss(y_pred, y)\n",
    "```\n",
    "- x: 입력벡터, y: 타깃이다. 입력데이터 x와 y가 고정되어 있다면 이 함수는 다음과 같이 표현할 수 있다.\n",
    "```python \n",
    "loss_value = f(W) \n",
    "```\n",
    "\n",
    "\n",
    "- W 값에 따라 loss_value 값이 바뀐다. 즉, **gradient(f)(W0)는 W0**에서 함수 **f(W)=loss_value**의 그래디언트이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 확률적 경사 하강법\n",
    "\n",
    "- 신경망에서의 **경사 하강법**(GD, Gradient Descent)는 손실함수를 최소화하기 위해 반복해서 가중치 파라미터를 조정해 나가는 것이라고 할 수 있다.\n",
    "\n",
    "- 신경망에서는 층이 깊어질수록 가중치 파라미터의 수가 엄청나게 많아지기 때문에 수식을 풀어서 한번에 최적의 값을 찾을 수 없다.\n",
    "- 따라서, 일반적으로 **미니 배치 확률적 경사 하강법**(mini-batch stochastic gradient descent)을 사용한다. \n",
    "- 신경망처럼 고차원 공간에서는 지역 최소값(local minima)은 매우 드물게 나타나며, 대부분 안장점(saddle point)로 나타난다.\n",
    "- 다양한 경사 하강법을 이용하여 손실함수를 최소화 시키는 방법을 최적화 방법(optimization)이라고 한다.\n",
    "- 옵티마이저에는 SGD, Adagrad, RMSProp 등 다양한 옵티마이저가 있다.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/op.gif\" height=\"60%\" width=\"60%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 변화율 연결: 역전파 알고리즘\n",
    "\n",
    "- 순전파를 통해 네트워크의 출력층에서 예측값(y_pred)을 출력 한다.\n",
    "\n",
    "```python\n",
    "y_perd = dot(W, x)\n",
    "loss_value = loss(y_pred, y)\n",
    "```\n",
    "\n",
    "- 이를 손실함수를 이용해 오차를 계산한 뒤, 역방향으로 거슬러 올라가면서 각 뉴런의 입력값에 대한 손실함수의 편미분을 계산한다(backpropagation).\n",
    "\n",
    "- 손실 값에 각 파라미터가 기여한 정도를 계산하기 위해 경사 하강법을 이용해 가중치를 조정하는 방법이 역전파 알고리즘이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 요약\n",
    "\n",
    "- **학습(learning)**은 학습 데이터 샘플과 그에 상응하는 타깃이 주어졌을 때 손실 함수를 최소화 하는 모델 파라미터의 조합을 찾는 것을 말한다.\n",
    "\n",
    "- 데이터 샘플과 타깃의 배치를 랜덤하게 추출하여 이 배치에서 손실에 대한 파라미터의 그래디언트를 계산함으로써 학습이 진행된다. 가중치 파라미터는 학습률(learning rate)에 의해 그래디언트 반대 방향으로 조금씩 움직인다.\n",
    "\n",
    "- 전체 학습 과정은 신경망이 미분 가능한 텐서 연산으로 연결되어 있기 때문에 가능하다. \n",
    "\n",
    "- 손실(loss)은 학습하는 동안 최소화해야 할 측정 지표이다.\n",
    "\n",
    "- 옵티마이저(optimizer)는 손실에 대한 그래디언트가 파라미터를 업데이트하는 방식을 정의한 것이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
